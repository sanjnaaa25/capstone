# -*- coding: utf-8 -*-
"""Clarity_and_Completeness_Scoring.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wiTa9gXzApwztvLlM7A3iB-ppQop7RcO
"""

!pip install transformers datasets torch scikit-learn nltk language_tool_python sentence-transformers

import nltk
nltk.download('punkt')  # for sentence splitting

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import language_tool_python
from sentence_transformers import SentenceTransformer, util
import numpy as np

from datasets import load_dataset

dataset = load_dataset("glue", "cola")

# For demonstration, simulate fine-grained clarity scores 0-10
import random
def add_score(example):
    if example['label'] == 1:  # Good grammar
        example['score'] = random.uniform(7, 10)
    else:  # Bad grammar
        example['score'] = random.uniform(0, 6)
    return example

dataset = dataset.map(add_score)

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize(batch):
    tokenized_inputs = tokenizer(
        batch['sentence'],
        padding='max_length', # Use max_length padding
        truncation=True,
        max_length=128 # Explicitly set max_length
    )
    tokenized_inputs['labels'] = batch['score']
    return tokenized_inputs

dataset = dataset.map(tokenize, batched=True)
dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=1)

!pip install --upgrade transformers

import transformers
print(transformers.__version__)

import torch.nn as nn
from transformers import Trainer, TrainingArguments

class RegressionTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels") # Corrected to 'labels'
        outputs = model(**inputs)
        logits = outputs.logits.squeeze(-1)
        loss_fct = nn.MSELoss()
        loss = loss_fct(logits, labels.float())
        return (loss, outputs) if return_outputs else loss

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="steps",   # works after upgrade
    save_strategy="steps",
    logging_steps=100,
    save_steps=200,
    report_to="none",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    learning_rate=2e-5,
    weight_decay=0.01,
)

trainer = RegressionTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"]
)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    learning_rate=2e-5,
    weight_decay=0.01,
    eval_strategy="steps",   # works after upgrade
    save_strategy="steps",
    logging_steps=100,
    save_steps=200,
    report_to="none"
)

trainer = RegressionTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"]
)

trainer.train()

import torch

def predict_score(texts, tokenizer, model):
    """
    Input: list of sentences
    Output: list of predicted scores (0-10)
    """
    model.eval()
    inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="pt")

    # Move inputs to the same device as the model
    device = model.device
    inputs = {key: value.to(device) for key, value in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits.squeeze(-1)

    return logits.cpu().numpy().tolist() # Move logits back to CPU for numpy conversion

sample_texts = [
    "I have completed all the points requested in the answer.",
    "The answer is is okay but missing key details and unclear in some parts."
]

scores = predict_score(sample_texts, tokenizer, model)
for text, score in zip(sample_texts, scores):
    print(f"Text: {text}")
    print(f"Predicted Score (0-10): {score:.2f}")
    print("-"*50)

# Install OpenJDK 17
!apt-get update
!apt-get install openjdk-17-jdk -y

import language_tool_python
# Download language models
tool = language_tool_python.LanguageTool('en-US')

# Optional: sentence embeddings for completeness check
from sentence_transformers import SentenceTransformer, util
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

reference_points = [
    "Include all required details",
    "Answer should be coherent",
    "Good grammar and clarity"
]
ref_embeddings = embed_model.encode(reference_points, convert_to_tensor=True)

def feedback(text, score):
    # Grammar feedback
    matches = tool.check(text)
    grammar_issues = len(matches)

    # Completeness feedback (semantic similarity)
    text_embedding = embed_model.encode(text, convert_to_tensor=True)
    similarities = util.cos_sim(text_embedding, ref_embeddings)
    completeness_score = float(similarities.max())

    print(f"Score: {score:.2f}/10")
    print(f"Grammar issues: {grammar_issues}")
    print(f"Completeness similarity: {completeness_score:.2f}")

    if score < 6:
        print("Tip: Improve grammar and include all key points in your answer.")
    elif score < 8:
        print("Tip: Answer is good but can be more detailed and clear.")
    else:
        print("Tip: Excellent answer!")
    print("-"*50)

for text, score in zip(sample_texts, scores):
    print(f"Text: {text}")
    feedback(text, score)

!pip install --upgrade transformers datasets

import transformers, datasets
print("Transformers:", transformers.__version__)
print("Datasets:", datasets.__version__)